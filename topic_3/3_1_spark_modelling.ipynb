{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d14f4cb-bf43-47c2-8607-0574876d23e9",
   "metadata": {},
   "source": [
    "# Applied Project in Big Data on Industrial Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1319da-a322-44d1-8335-b282621964ab",
   "metadata": {},
   "source": [
    "## MODELING\n",
    "## Part I. Spark modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebb616a-d94e-41ab-b3bd-3a1b17c85ba5",
   "metadata": {},
   "source": [
    "### 1. Libraries and Spark setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a78ba91-a8ad-43c3-806e-5fc23c39a35b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd2c2d3-3816-423f-b760-a70937e6b478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PRJ_PATH = '/home/jovyan/__RAYPFP24'\n",
    "\n",
    "\n",
    "def access_data(file_path):\n",
    "    with open(file_path) as file:\n",
    "        access_data = json.load(file)\n",
    "    return access_data\n",
    "\n",
    "\n",
    "access_s3_data = access_data(f'{PRJ_PATH}/.access_jhub_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f214c4d-2142-4557-814f-4fb22a0370d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def uiWebUrl(self):\n",
    "    from urllib.parse import urlparse\n",
    "    web_url = self._jsc.sc().uiWebUrl().get()\n",
    "    port = urlparse(web_url).port\n",
    "    return '{}proxy/{}/jobs/'.format(os.environ['JUPYTERHUB_SERVICE_PREFIX'], port)\n",
    "\n",
    "\n",
    "SparkContext.uiWebUrl = property(uiWebUrl)\n",
    "conf = SparkConf()\n",
    "conf.set('spark.master', 'local[*]')\n",
    "conf.set('spark.driver.memory', '40G')\n",
    "conf.set('spark.driver.maxResultSize', '8G')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.access.key', access_s3_data['aws_access_key_id'])\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.secret.key', access_s3_data['aws_secret_access_key'])\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.multipart.size', '104857600')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.block.size', '33554432')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.threads.max', '256')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.endpoint', 'http://storage.yandexcloud.net')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.aws.credentials.provider', \n",
    "                                     'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8ba75-8dcf-418c-9019-dae245606403",
   "metadata": {},
   "source": [
    "### 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6671005-60e1-471a-9510-60ae3dad10b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VER = 'v0'\n",
    "PROC_DS = False\n",
    "PROC_LAGS = True\n",
    "FRAC_0 = .002  # used only if `PROC_LAGS = True`\n",
    "PROC_VECS = True\n",
    "BUCKET = access_s3_data['bucket_name']\n",
    "REPART = 1\n",
    "W2V = True\n",
    "\n",
    "files_path = 'data/events'\n",
    "files_mask = f'{files_path}/data_2023-*-*.csv'\n",
    "\n",
    "file_path_ds = f's3a://{BUCKET}/work/{VER}/data_raw.parquet'\n",
    "file_path_lags = f's3a://{BUCKET}/work/{VER}/data_lags.parquet'\n",
    "file_path_trn = f's3a://{BUCKET}/work/{VER}/data_vec_train.parquet'\n",
    "file_path_tst = f's3a://{BUCKET}/work/{VER}/data_vec_test.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70582610-f1b6-4083-be79-655baf34f001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_parquet(path):\n",
    "    cmd = path.replace(\n",
    "        f's3a://{BUCKET}',\n",
    "        f'rm -rf {PRJ_PATH}'\n",
    "    )\n",
    "    !{cmd}\n",
    "    return f'command to run: {cmd}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1718da-1149-4422-9160-9a96d3ea1be7",
   "metadata": {},
   "source": [
    "#### 2.1. Load or preprocess data - `raw` stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec139b21-815a-4680-a53e-c43a5e24d355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "flag_min_datetime = datetime.datetime(2023, 8, 1, 0, 0, 0)\n",
    "flag_max_datetime = datetime.datetime(2023, 8, 7, 23, 59, 59)\n",
    "print(\n",
    "    'from', flag_min_datetime, \n",
    "    'to', flag_max_datetime\n",
    ")\n",
    "\n",
    "if PROC_DS:\n",
    "    sdf = spark.read.option('escape','\"').csv(f's3a://{BUCKET}/{files_mask}', header=True)\n",
    "    sdf = sdf.withColumn('event_datetime', F.to_timestamp(\"event_datetime\"))\n",
    "    sdf = sdf.withColumn(\n",
    "        'payment_event_flag', \n",
    "        (\n",
    "            (F.col('event_name').like('%Мои штрафы/Оплата/Завершили оплату%') | \n",
    "            F.col('event_name').like('%Мои штрафы/Оплата/Платёж принят%')) &\n",
    "            F.col('event_datetime').between(flag_min_datetime, flag_max_datetime)\n",
    "        ).cast(\"int\")\n",
    "    )\n",
    "    sdf = sdf.select(\n",
    "        'profile_id',\n",
    "        'event_datetime',\n",
    "        'payment_event_flag',\n",
    "        'event_name'\n",
    "    )\n",
    "    cmd = file_path_ds.replace(\n",
    "        f's3a://{BUCKET}',\n",
    "        f'ls -la {PRJ_PATH}'\n",
    "    )\n",
    "    clean_parquet(file_path_ds)\n",
    "    sdf.repartition(REPART).write.parquet(file_path_ds)\n",
    "    sdf.unpersist()\n",
    "\n",
    "sdf = spark.read.parquet(file_path_ds)\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22c54c5-820c-4822-825f-4557981ad2b6",
   "metadata": {},
   "source": [
    "### 2.2. Load or preprocess data - `lags` stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8ce99e-a387-498c-83b7-ed30e8845234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dataset_lags(sdf, shift=0):\n",
    "    hour = 60 * 60\n",
    "    day = 24 * 60 * 60\n",
    "    w_10min_to_1hour = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-1 * hour + shift, -10 * 60 + shift))\n",
    "    w_1_to_24hours = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-24 * hour + shift, -hour + shift))\n",
    "    w_1day_to_3days = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-3 * day + shift, -day + shift))\n",
    "    w_3days_to_7days = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-7 * day + shift, -3 * day + shift))\n",
    "    w_7days_to_15days = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-15 * day + shift, -7 * day + shift))\n",
    "    w_15days_to_30days = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-30 * day + shift, -15 * day + shift))\n",
    "    w_1mth_to_2mth = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-2 * 30 * day + shift, -1 * 30 * day + shift))\n",
    "    w_2mth_to_3mth = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-3 * 30 * day + shift, -2 * 30 * day + shift))\n",
    "    w_3mth_to_4mth = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-4 * 30 * day + shift, -3 * 30 * day + shift))\n",
    "    w_4mth_to_5mth = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-5 * 30 * day + shift, -4 * 30 * day + shift))\n",
    "    w_5mth_to_6mth = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-6 * 30 * day + shift, -5 * 30 * day + shift))\n",
    "    return (\n",
    "        sdf\n",
    "            .withColumn('lag_10min_to_1hour', F.collect_list('event_name').over(w_10min_to_1hour))\n",
    "            .withColumn('lag_1_to_24hours', F.collect_list('event_name').over(w_1_to_24hours))\n",
    "            .withColumn('lag_1day_to_3days', F.collect_list('event_name').over(w_1day_to_3days))\n",
    "            .withColumn('lag_3days_to_7days', F.collect_list('event_name').over(w_3days_to_7days))\n",
    "            #.withColumn('lag_7days_to_15days', F.collect_list('event_name').over(w_7days_to_15days))\n",
    "            #.withColumn('lag_15days_to_30days', F.collect_list('event_name').over(w_15days_to_30days))\n",
    "            #.withColumn('lag_1mth_to_2mth', F.collect_list('event_name').over(w_1mth_to_2mth))\n",
    "            #.withColumn('lag_2mth_to_3mth', F.collect_list('event_name').over(w_2mth_to_3mth))\n",
    "            #.withColumn('lag_3mth_to_4mth', F.collect_list('event_name').over(w_3mth_to_4mth))\n",
    "            #.withColumn('lag_4mth_to_5mth', F.collect_list('event_name').over(w_4mth_to_5mth))\n",
    "            #.withColumn('lag_5mth_to_6mth', F.collect_list('event_name').over(w_5mth_to_6mth))\n",
    "            .select(\n",
    "                'profile_id',\n",
    "                'event_datetime',\n",
    "                'payment_event_flag',\n",
    "                'event_name',\n",
    "                'lag_10min_to_1hour',\n",
    "                'lag_1_to_24hours',\n",
    "                'lag_1day_to_3days',\n",
    "                'lag_3days_to_7days',\n",
    "                #'lag_7days_to_15days',\n",
    "                #'lag_15days_to_30days',\n",
    "                #'lag_1mth_to_2mth',\n",
    "                #'lag_2mth_to_3mth',\n",
    "                #'lag_3mth_to_4mth',\n",
    "                #'lag_4mth_to_5mth',\n",
    "                #'lag_5mth_to_6mth'\n",
    "            )\n",
    "        .orderBy(F.col('event_datetime'), ascending=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1096c4e5-1885-4cd3-adde-38d6e407a93c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if PROC_LAGS:\n",
    "    sdf = sdf.sampleBy(\n",
    "        'payment_event_flag', \n",
    "        fractions={0: FRAC_0, 1: 1}, \n",
    "        seed=2023\n",
    "    )\n",
    "    sdf = dataset_lags(sdf)\n",
    "    dates  = (flag_min_datetime, flag_max_datetime)\n",
    "    sdf = sdf.filter(sdf.event_datetime.between(*dates))\n",
    "    sdf = sdf.filter(\n",
    "        (F.size('lag_10min_to_1hour')   > 0) |\n",
    "        (F.size('lag_1_to_24hours')     > 0) |\n",
    "        (F.size('lag_1day_to_3days')    > 0) |\n",
    "        (F.size('lag_3days_to_7days')   > 0)\n",
    "        #(F.size('lag_7days_to_15days')  > 0) |\n",
    "        #(F.size('lag_15days_to_30days') > 0) |\n",
    "        #(F.size('lag_1mth_to_2mth')     > 0) |\n",
    "        #(F.size('lag_2mth_to_3mth')     > 0) |\n",
    "        #(F.size('lag_3mth_to_4mth')     > 0) |\n",
    "        #(F.size('lag_4mth_to_5mth')     > 0) |\n",
    "        #(F.size('lag_5mth_to_6mth')     > 0)\n",
    "    )\n",
    "    clean_parquet(file_path_lags)\n",
    "    sdf.repartition(REPART).write.parquet(file_path_lags)\n",
    "    sdf.unpersist()\n",
    "\n",
    "sdf = spark.read.parquet(file_path_lags)\n",
    "sdf.groupBy('payment_event_flag').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de83425-0fbd-46bc-978a-cfad7ce0c311",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stratified_split(sdf, frac, label, seed=2023):\n",
    "    zeros = sdf.filter(sdf[label] == 0)\n",
    "    ones = sdf.filter(sdf[label] == 1)\n",
    "    train_, test_ = zeros.randomSplit([1 - frac, frac], seed=seed)\n",
    "    train, test = ones.randomSplit([1 - frac, frac], seed=seed)\n",
    "    train = train.union(train_)\n",
    "    test = test.union(test_)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa94f255-3f3a-453b-aad9-8772ce8f5a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sdf_train, sdf_test = stratified_split(\n",
    "    sdf,\n",
    "    frac=.2,\n",
    "    label='payment_event_flag',\n",
    "    seed=2023\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e35b0-bd8f-4318-a973-155b4fd72a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sdf_train.groupBy('payment_event_flag').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8c012b-f4ec-46bd-a0c3-7270d31d6b14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sdf_test.groupBy('payment_event_flag').count().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9856ef-8571-467e-871f-70cbf66e6e6e",
   "metadata": {},
   "source": [
    "### 2.3. Load or preprocess data - `vectorize` stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b8a249-c54f-493f-9679-baaa4e3dff6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lags = [\n",
    "    'lag_10min_to_1hour',\n",
    "    'lag_1_to_24hours',\n",
    "    'lag_1day_to_3days',\n",
    "    'lag_3days_to_7days',\n",
    "    #'lag_7days_to_15days',\n",
    "    #'lag_15days_to_30days',\n",
    "    #'lag_1mth_to_2mth',\n",
    "    #'lag_2mth_to_3mth',\n",
    "    #'lag_3mth_to_4mth',\n",
    "    #'lag_4mth_to_5mth',\n",
    "    #'lag_5mth_to_6mth'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87fccc4-837d-495b-8d14-38c3ce8d69c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def datasets_vecorized(sdf_train, sdf_test, lags, vec_size=10):\n",
    "    vectorizers = []\n",
    "    for lag in tqdm(lags):\n",
    "        word2Vec = Word2Vec(\n",
    "            vectorSize=vec_size,\n",
    "            minCount=0,\n",
    "            inputCol=lag,\n",
    "            outputCol=lag + '_vec'\n",
    "        )\n",
    "        vectorizer = word2Vec.fit(sdf_train)\n",
    "        sdf_train = vectorizer.transform(sdf_train)\n",
    "        sdf_test = vectorizer.transform(sdf_test)\n",
    "        vectorizers.append(vectorizer)\n",
    "    return sdf_train, sdf_test, vectorizers\n",
    "\n",
    "\n",
    "def datasets_tfidf(sdf_train, sdf_test, lags, min_freq=3, num_features=10):\n",
    "    \"\"\"\n",
    "    Good explanation is here:\n",
    "    https://www.analyticsvidhya.com/blog/2022/09/implementing-count-vectorizer-and-tf-idf-in-nlp-using-pyspark/\n",
    "\n",
    "    \"\"\"\n",
    "    idfmodels = {}\n",
    "    features_dict = {}\n",
    "    count = 0\n",
    "    for lag in tqdm(lags):\n",
    "        hashingTF = HashingTF(\n",
    "            inputCol=lag,\n",
    "            outputCol=lag + '_tf',\n",
    "            numFeatures=num_features\n",
    "        )\n",
    "        featurizedData = hashingTF.transform(sdf_train)\n",
    "        idf = IDF(\n",
    "            inputCol=lag + '_tf',\n",
    "            outputCol=lag + '_tfidf',\n",
    "            minDocFreq=min_freq\n",
    "        )\n",
    "        idfModel = idf.fit(featurizedData)\n",
    "        sdf_train = idfModel.transform(featurizedData)\n",
    "        sdf_test = idfModel.transform(\n",
    "            hashingTF.transform(sdf_test)\n",
    "        )\n",
    "        idfmodels[lag] = idfModel\n",
    "        events = [\n",
    "            x\n",
    "            for xs in sdf_train.select(lag).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "            for x in xs\n",
    "        ]\n",
    "        hash_dict = {}\n",
    "        for e in events:\n",
    "            hash_dict[lag + '_' + e] = hashingTF.indexOf(e)\n",
    "        for feat_num in range(num_features):\n",
    "            tmp_list = []\n",
    "            for k, v in hash_dict.items():\n",
    "                if v == feat_num: tmp_list.append(k)\n",
    "            features_dict[count * num_features + feat_num] = tmp_list\n",
    "        count += 1\n",
    "    return sdf_train, sdf_test, features_dict, idfmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84729a95-0c69-43fa-bc61-a0955645720f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if PROC_VECS:\n",
    "    if W2V:\n",
    "        sdf_train, sdf_test, vectorizers = datasets_vecorized(\n",
    "            sdf_train,\n",
    "            sdf_test,\n",
    "            lags,\n",
    "            vec_size=10\n",
    "        )\n",
    "    else:\n",
    "        sdf_train, sdf_test, features_dict, idfmodels = datasets_tfidf(\n",
    "            sdf_train,\n",
    "            sdf_test,\n",
    "            lags,\n",
    "            min_freq=3,\n",
    "            num_features=100\n",
    "        )\n",
    "        print('tfidf len features:', len(features_dict.items()))\n",
    "    clean_parquet(file_path_trn)\n",
    "    sdf_train.repartition(REPART).write.parquet(file_path_trn)\n",
    "    clean_parquet(file_path_tst)\n",
    "    sdf_test.repartition(REPART).write.parquet(file_path_tst)\n",
    "    sdf_train.unpersist()\n",
    "    sdf_test.unpersist()\n",
    "\n",
    "sdf_train = spark.read.parquet(file_path_trn)\n",
    "sdf_test = spark.read.parquet(file_path_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158b2590-13e2-4cce-b5b7-6232a21fe981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sdf_train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c6b9e0-0e7b-4459-ab37-e970f26fb3d6",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd83edd-91ab-4364-b7a3-64abcd9def7f",
   "metadata": {},
   "source": [
    "### 3.1. Features assembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f662437f-0848-4e87-83f6-28edba536c75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def features_assembled(sdf, feats, postfix='_vec'):\n",
    "    cols_to_model = [x + postfix for x in feats]\n",
    "    cols_to_model.extend(['payment_event_flag'])\n",
    "    print('columns to model:', cols_to_model)\n",
    "    vecAssembler = VectorAssembler(\n",
    "        inputCols=[c for c in cols_to_model if c != 'payment_event_flag'], \n",
    "        outputCol='features'\n",
    "    )\n",
    "    features = sdf.select(cols_to_model)\n",
    "    features_vec = vecAssembler.transform(features)\n",
    "    features_data = features_vec.select('payment_event_flag', 'features')\n",
    "    return features_data\n",
    "\n",
    "\n",
    "def upsampled(sdf, label, upsample='max'):\n",
    "    zeros = sdf.filter(sdf[label] == 0)\n",
    "    ones = sdf.filter(sdf[label] == 1)\n",
    "    res = zeros.union(ones)\n",
    "    if upsample == 'max':\n",
    "        up_count = int(zeros.count() / ones.count())\n",
    "        for _ in range(up_count - 1):\n",
    "            res = res.union(ones)\n",
    "    else:\n",
    "        for _ in range(upsample - 1):\n",
    "            res = res.union(ones)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29bfde6-70c1-4272-8bc6-0e317a673776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "UPSAMPLE = None  # can be None or 'max'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefefcd5-8048-4120-93fa-9687290ada7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feats = [\n",
    "    'lag_10min_to_1hour',\n",
    "    'lag_1_to_24hours',\n",
    "    'lag_1day_to_3days',\n",
    "    'lag_3days_to_7days',\n",
    "    #'lag_7days_to_15days',\n",
    "    #'lag_15days_to_30days',\n",
    "    #'lag_1mth_to_2mth',\n",
    "    #'lag_2mth_to_3mth',\n",
    "    #'lag_3mth_to_4mth',\n",
    "    #'lag_4mth_to_5mth',\n",
    "    #'lag_5mth_to_6mth'\n",
    "]\n",
    "if W2V:\n",
    "    postfix = '_vec'\n",
    "else:\n",
    "    postfix = '_tfidf'\n",
    "features_train = features_assembled(sdf_train, feats=feats, postfix=postfix)\n",
    "features_test = features_assembled(sdf_test, feats=feats, postfix=postfix)\n",
    "if UPSAMPLE:\n",
    "    features_train = upsampled(\n",
    "        features_train,\n",
    "        label='payment_event_flag',\n",
    "        upsample=UPSAMPLE\n",
    "    )\n",
    "    # Use to upsample test set\n",
    "    features_test = upsampled(\n",
    "        features_test,\n",
    "        label='payment_event_flag',\n",
    "        upsample=UPSAMPLE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388d38ed-6ced-4935-9762-3d0941a29e58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_train.groupBy('payment_event_flag').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f2dfc9-ed4f-4139-9074-c986b12cdc7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_test.groupBy('payment_event_flag').count().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad4814c-5ddf-46e7-bea6-03c45f94cac7",
   "metadata": {},
   "source": [
    "### 3.2. Training and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9167c4-b21e-4ccc-8004-570635f27ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    labelCol='payment_event_flag',\n",
    "    featuresCol='features',\n",
    "    numTrees=100,\n",
    "    maxDepth=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f90b6a-6bd3-469f-9052-1c1e1a52fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = rf.fit(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5f761a-bc67-45e9-a6d4-cce0e0771604",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(features_test)\n",
    "payment_event_flag_preds = predictions.select('prediction', 'payment_event_flag')\n",
    "metrics = BinaryClassificationMetrics(\n",
    "    payment_event_flag_preds.rdd.map(\n",
    "        lambda lines: [float(x) for x in lines]\n",
    "    )\n",
    ")\n",
    "print('ROC AUC:', metrics.areaUnderROC)\n",
    "print('Area under PR-curve:', metrics.areaUnderPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac979e-ebc3-4bf8-ad41-baa3bd95faa3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3. Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bdaf8e-abd9-45c3-baf3-00552b1460f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TH = .01\n",
    "\n",
    "features_imps = {}\n",
    "for i, v in enumerate(model.featureImportances.toArray()):\n",
    "    if v >= TH: features_imps[i] = v\n",
    "features_imps = dict(sorted(features_imps.items(), key=lambda x: x[1], reverse=True))\n",
    "features_imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4151f8-a816-462d-b1d1-62998fb77aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if W2V:\n",
    "    pass\n",
    "else:\n",
    "    for k, v in features_imps.items():\n",
    "        print('-' * 100)\n",
    "        print('feature number:', k, '| feature importance:', v)\n",
    "        print('features:', features_dict[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3059d6-0038-4504-bcc6-f5792217d842",
   "metadata": {},
   "source": [
    "### 3.3. Future look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6714aea4-840b-40b5-8370-1b0fa7abca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_pred = spark.read.parquet(file_path_ds)\n",
    "sdf_pred.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78669fa1-926a-43a0-83de-4621a2dc1b62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "SHIFT = 2 * 24 * 60 * 60  # 2 days ahead\n",
    "\n",
    "sdf_pred = sdf_pred.sample(fraction=.0001)\n",
    "sdf_pred = dataset_lags(sdf_pred, shift=SHIFT)\n",
    "sdf = sdf.filter(\n",
    "    (F.size('lag_10min_to_1hour')   > 0) |\n",
    "    (F.size('lag_1_to_24hours')     > 0) |\n",
    "    (F.size('lag_1day_to_3days')    > 0) |\n",
    "    (F.size('lag_3days_to_7days')   > 0)\n",
    "    #(F.size('lag_7days_to_15days')  > 0) |\n",
    "    #(F.size('lag_15days_to_30days') > 0) |\n",
    "    #(F.size('lag_1mth_to_2mth')     > 0) |\n",
    "    #(F.size('lag_2mth_to_3mth')     > 0) |\n",
    "    #(F.size('lag_3mth_to_4mth')     > 0) |\n",
    "    #(F.size('lag_4mth_to_5mth')     > 0) |\n",
    "    #(F.size('lag_5mth_to_6mth')     > 0)\n",
    ")\n",
    "sdf_pred.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1bd830-4ead-4da7-aa76-859851cd32d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70de6eb7-f7be-43c0-83d4-804a4ba596f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if W2V:\n",
    "    for i, lag in enumerate(lags):\n",
    "        sdf_pred = vectorizers[i].transform(sdf_pred)\n",
    "        print(lag, '-> vec done')\n",
    "else:\n",
    "    for i, lag in enumerate(lags):\n",
    "        hashingTF = HashingTF(\n",
    "            inputCol=lag,\n",
    "            outputCol=lag + '_tf',\n",
    "            numFeatures=100\n",
    "        )\n",
    "        sdf_pred = idfmodels[lag].transform(\n",
    "            hashingTF.transform(sdf_pred)\n",
    "        )\n",
    "        print(lag, '-> idf done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec04052-748d-45df-9a97-31bafcff6e02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_pred = features_assembled(sdf_pred, feats=feats, postfix=postfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8be7484-19d8-4ae9-88bd-56c1c30f6483",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_future = model.transform(features_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c1309a-fab6-4cf8-8807-5d41862cac6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_pred = sdf_pred.select(sdf_pred.profile_id).toPandas()\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff3d324-494b-4f86-a80c-c3a4c239c610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_predictions_future = predictions_future.withColumn(\n",
    "    'tmp',\n",
    "    vector_to_array('probability')\n",
    ").select(\n",
    "    F.col('tmp')[1].alias('prob_next7days')\n",
    ").toPandas()\n",
    "df_predictions_future.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d450d7-9230-4287-9c96-2f08a3e81474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_predictions_future.filter(\n",
    "    df_predictions_future.prob_next7days > .5\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cac821-fbaf-4d3f-a2de-d14df3741e98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path_preds = f'{PRJ_PATH}/work/{VER}/preds.csv'\n",
    "df_pred.join(df_predictions_future).to_csv(file_path_preds, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984b19d8-2b59-4b6f-a6e0-9751e0494fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
