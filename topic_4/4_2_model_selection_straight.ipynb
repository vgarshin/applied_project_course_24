{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "separated-repair",
   "metadata": {},
   "source": [
    "# Applied Project in Big Data on Industrial Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-convention",
   "metadata": {},
   "source": [
    "## MODELS SELECTION TECHNIQUES\n",
    "## Part II. Straight way to organize a project and track experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-immune",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from sklearn.feature_extraction.text import (\n",
    "    TfidfVectorizer, \n",
    "    CountVectorizer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score, \n",
    "    confusion_matrix, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    roc_curve, \n",
    "    auc\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, \n",
    "    GradientBoostingClassifier\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_score, \n",
    "    train_test_split,\n",
    "    StratifiedKFold\n",
    ")\n",
    "pd.set_option('display.max_columns', None)\n",
    "N_CORES = min(\n",
    "    multiprocessing.cpu_count(), \n",
    "    int(float(os.environ['CPU_LIMIT']))\n",
    ")\n",
    "print('cores:', N_CORES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-shoot",
   "metadata": {},
   "source": [
    "### 2. Create config and place to store artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39adf4d-173c-4bb2-8b64-3cf3b96d9bda",
   "metadata": {},
   "source": [
    "It would be a good idea to store all experiment's artifacts in one place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# here is our config dictionary\n",
    "# we can use it to manage model's parameters\n",
    "# and save it to disk for a history\n",
    "VER = 'v0'\n",
    "CONFIG = {\n",
    "    'version': VER,\n",
    "    'start_time': str(datetime.datetime.fromtimestamp(start_time)),\n",
    "    'sample_size': 1500,\n",
    "    'ngram_range': (1, 1), \n",
    "    'max_df': .95, \n",
    "    'min_df': 5,\n",
    "    'clf': 'GradientBoostingClassifier', # 'RandomForestClassifier' or `GradientBoostingClassifier`\n",
    "    'folds': 4,\n",
    "    'seed': 2023,\n",
    "    'n_iters': 10,\n",
    "    'comments': 'my first model'\n",
    "}\n",
    "\n",
    "# path to store our model\n",
    "# will create folder each time\n",
    "# we run our training code\n",
    "MDLS_PATH = f'./models_{VER}'\n",
    "if not os.path.exists(MDLS_PATH):\n",
    "    os.mkdir(MDLS_PATH)\n",
    "with open(f'{MDLS_PATH}/config.json', 'w') as file:\n",
    "    json.dump(CONFIG, file)\n",
    "\n",
    "# useful trick to fix randomness\n",
    "def seed_all(seed):\n",
    "    \"\"\"\n",
    "    Sometimes it is useful to nail all randomness\n",
    "    and fix all random seeds for reproducibility.\n",
    "    \n",
    "    This function fixes all random seeds for current pipline, \n",
    "    but it could be extended e.g. for Tensorflow library \n",
    "    you may want to add `tf.random.set_seed(seed)` in the code.\n",
    "    \n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_all(CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-virtue",
   "metadata": {},
   "source": [
    "### 2. Dataset for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('articles_data.csv')\n",
    "df = df.sample(CONFIG['sample_size']).reset_index()\n",
    "del df['index']\n",
    "print(df.shape)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d37ca9d-5006-4dee-91b9-80d332c766aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('target').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not necessary but can be helpful\n",
    "# to reproduce experiments\n",
    "save_data_path = f'{MDLS_PATH}/data_{CONFIG[\"version\"]}.csv'\n",
    "df.to_csv(save_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-elements",
   "metadata": {},
   "source": [
    "### 3. Modelling with save of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_features(data, vectorizer):\n",
    "    print('total texts:', len(data))\n",
    "    features = vectorizer.fit_transform(data)\n",
    "    print(\n",
    "        'features shape:', features.shape, \n",
    "        'max:', np.max(features), \n",
    "        'min:', np.min(features)\n",
    "    )\n",
    "    return features, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_model(X, y, \n",
    "                    folds, clf,\n",
    "                    vectorizer, ngram_range=(1, 1), \n",
    "                    max_df=.2, min_df=8, seed=2022):\n",
    "    scores = {}\n",
    "    roc_auc_scores = []\n",
    "    f1_scores = []\n",
    "    skf = StratifiedKFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "    for fold, (train_idxs, test_idxs) in enumerate(skf.split(X, y)):\n",
    "        \n",
    "        # train model\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_idxs], X.iloc[test_idxs]\n",
    "        y_train, y_test = y.iloc[train_idxs], y.iloc[test_idxs]\n",
    "        X_train, vectorizer = text_features(\n",
    "            X_train, \n",
    "            vectorizer=vectorizer\n",
    "        )\n",
    "        X_test = vectorizer.transform(X_test)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # saving models\n",
    "        # more about https://scikit-learn.org/stable/model_persistence.html\n",
    "        # NOTE - not only model, but vectorizer too!\n",
    "        \n",
    "        file_name = f'{MDLS_PATH}/model_fold_{fold}.joblib'\n",
    "        dump(clf, file_name)\n",
    "        print('saved to', file_name)\n",
    "        file_name = f'{MDLS_PATH}/vectorizer_fold_{fold}.joblib'\n",
    "        dump(vectorizer, file_name)\n",
    "        print('saved to', file_name)\n",
    "        \n",
    "        # metrics\n",
    "        \n",
    "        y_score = clf.predict_proba(X_test)\n",
    "        roc_auc_score_ = roc_auc_score(y_test, y_score[:, 1])\n",
    "        roc_auc_scores.append(roc_auc_score_)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        f1_score_ = f1_score(y_test, y_pred)\n",
    "        f1_scores.append(f1_score_)\n",
    "        msg = f'fold {fold} - val ROC-AUC score: {roc_auc_score_:.2f}, val f1-score: {f1_score_:.2f}'\n",
    "        print(msg)\n",
    "        \n",
    "        scores[f'fold {fold}'] = {\n",
    "            'roc_auc_scores': roc_auc_scores,\n",
    "            'f1_scores': f1_scores\n",
    "        }\n",
    "        \n",
    "        # logging (if needed)\n",
    "        # read about logging for production cases here https://docs.python.org/3/library/logging.html\n",
    "        \n",
    "        log_file_name = f'{MDLS_PATH}/model_{CONFIG[\"version\"]}.log'\n",
    "        log_string = f'{str(datetime.datetime.fromtimestamp(time.time()))} - {msg}\\n'\n",
    "        with open(log_file_name, 'a') as file:\n",
    "            file.write(log_string)\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vectorizer with respect to\n",
    "# our configuration parameters\n",
    "vectorizer=TfidfVectorizer(\n",
    "    ngram_range=CONFIG['ngram_range'], \n",
    "    max_df=CONFIG['max_df'], \n",
    "    min_df=CONFIG['min_df']\n",
    ")\n",
    "\n",
    "# select the type of the model\n",
    "if CONFIG['clf'] == 'RandomForestClassifier':\n",
    "    clf = RandomForestClassifier(n_estimators=CONFIG['n_iters']) \n",
    "elif CONFIG['clf'] == 'GradientBoostingClassifier':\n",
    "    clf = GradientBoostingClassifier(n_estimators=CONFIG['n_iters']) \n",
    "else:\n",
    "    clf = LogisticRegression()\n",
    "    \n",
    "# start our training\n",
    "scores = cross_val_model(\n",
    "    X=df['proc'], \n",
    "    y=df['target'], \n",
    "    folds=CONFIG['folds'], \n",
    "    clf=clf,\n",
    "    vectorizer=vectorizer,\n",
    "    seed=CONFIG['seed']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b422cf2-eda4-4b17-919b-cac164b6b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-finding",
   "metadata": {},
   "source": [
    "### 4. Not only save models but get them back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-bacteria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore our config \n",
    "# from the disk\n",
    "VER = 'v0'\n",
    "with open(f'{MDLS_PATH}/config.json', 'r') as file:\n",
    "    config = json.load(file)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "vectorizers = []\n",
    "for fold in range(config['folds']):\n",
    "    # load trained models\n",
    "    file_name = f'{MDLS_PATH}/model_fold_{fold}.joblib'\n",
    "    model = load(file_name)\n",
    "    models.append(model)\n",
    "    \n",
    "    # that is why we saved vectorizers before\n",
    "    file_name = f'{MDLS_PATH}/vectorizer_fold_{fold}.joblib'\n",
    "    vectorizer = load(file_name)\n",
    "    vectorizers.append(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is not a good idea to test our models\n",
    "# on the data they were trained on\n",
    "# but it is just a demo of approach\n",
    "df_tmp = pd.read_csv('articles_data.csv')\n",
    "df_tmp = df_tmp.sample(100).reset_index()\n",
    "del df_tmp['index']\n",
    "print(df_tmp.shape)\n",
    "display(df_tmp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectorizer = vectorizers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_vectorizer.transform(df_tmp['proc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = test_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp['predictions'] = y_score[:, 1]\n",
    "print(df_tmp.shape)\n",
    "display(df_tmp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b634533-85e2-4bdf-ab19-47b1c262bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(df_tmp.target, df_tmp.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-apparatus",
   "metadata": {},
   "source": [
    "### 5. Way of Jedi datascientist - make a predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-daniel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jedi_predictions(data, models, vectorizers):\n",
    "    \"\"\"\n",
    "    Jedi function takes all the models and\n",
    "    makes predictions with all of them\n",
    "    and then returns average predictions.\n",
    "    \n",
    "    This approach usually gives a better\n",
    "    result than a single model predictions.\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for m, v in zip(models, vectorizers):\n",
    "        data_ = v.transform(data)\n",
    "        y_score = m.predict_proba(data_)\n",
    "        preds.append(y_score)\n",
    "    return np.mean(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-newsletter",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = jedi_predictions(df_tmp['proc'], models, vectorizers)\n",
    "y_score[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-saudi",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp['predictions'] = y_score[:, 1]\n",
    "print(df_tmp.shape)\n",
    "display(df_tmp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-committee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we expect to get higher metrics \n",
    "# than from a single model \n",
    "roc_auc_score(df_tmp.target, df_tmp.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-rings",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
