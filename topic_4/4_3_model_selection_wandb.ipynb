{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "separated-repair",
   "metadata": {},
   "source": [
    "# Applied Project in Big Data on Industrial Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-convention",
   "metadata": {},
   "source": [
    "## MODELS SELECTION TECHNIQUES\n",
    "## Part III. Weights and Biases platform to manage experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-immune",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-blend",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import wandb\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from sklearn.feature_extraction.text import (\n",
    "    TfidfVectorizer, \n",
    "    CountVectorizer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score, \n",
    "    confusion_matrix, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    roc_curve, \n",
    "    auc\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, \n",
    "    GradientBoostingClassifier\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_score, \n",
    "    train_test_split,\n",
    "    StratifiedKFold\n",
    ")\n",
    "pd.set_option('display.max_columns', None)\n",
    "N_CORES = min(\n",
    "    multiprocessing.cpu_count(), \n",
    "    int(float(os.environ['CPU_LIMIT']))\n",
    ")\n",
    "print('cores:', N_CORES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-shoot",
   "metadata": {},
   "source": [
    "### 2. Create config and place to store artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de68ac-cdb4-49d0-b079-a788da4164ec",
   "metadata": {},
   "source": [
    "We will expand our approach with use of [WandB](https://wandb.ai) platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# here is our config dictionary\n",
    "# we can use it to manage model's parameters\n",
    "# and save it to disk for a history\n",
    "VER = 'v0'\n",
    "CONFIG = {\n",
    "    'version': VER,\n",
    "    'start_time': str(datetime.datetime.fromtimestamp(start_time)),\n",
    "    'sample_size': 1000,\n",
    "    'ngram_range': (1, 1), \n",
    "    'max_df': .95, \n",
    "    'min_df': 5,\n",
    "    'clf': 'GradientBoostingClassifier', # 'RandomForestClassifier' or `GradientBoostingClassifier`\n",
    "    'folds': 4,\n",
    "    'seed': 2023,\n",
    "    'n_iters': 10,\n",
    "    'comments': 'my first model'\n",
    "}\n",
    "\n",
    "# path to store our model\n",
    "# will create folder each time\n",
    "# we run our training code\n",
    "MDLS_PATH = f'./models_{VER}'\n",
    "if not os.path.exists(MDLS_PATH):\n",
    "    os.mkdir(MDLS_PATH)\n",
    "with open(f'{MDLS_PATH}/config.json', 'w') as file:\n",
    "    json.dump(CONFIG, file)\n",
    "\n",
    "# useful trick to fix randomness\n",
    "def seed_all(seed):\n",
    "    \"\"\"\n",
    "    Sometimes it is useful to nail all randomness\n",
    "    and fix all random seeds for reproducibility.\n",
    "    \n",
    "    This function fixes all random seeds for current pipline, \n",
    "    but it could be extended e.g. for Tensorflow library \n",
    "    you may want to add `tf.random.set_seed(seed)` in the code.\n",
    "    \n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_all(CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-flour",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may run `wandb.login()` in notebook\n",
    "# or `wandb login` in terminal\n",
    "\n",
    "with open(f'/home/jovyan/.wandb', 'r') as file:\n",
    "    api_key = file.read()\n",
    "wandb.login(key=api_key.replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-virtue",
   "metadata": {},
   "source": [
    "### 2. Dataset for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('articles_data.csv')\n",
    "df = df.sample(CONFIG['sample_size']).reset_index()\n",
    "del df['index']\n",
    "print(df.shape)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-hawaiian",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('target').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-elements",
   "metadata": {},
   "source": [
    "### 3. Modelling with save of results with WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_features(data, vectorizer):\n",
    "    print('total texts:', len(data))\n",
    "    features = vectorizer.fit_transform(data)\n",
    "    print(\n",
    "        'features shape:', features.shape, \n",
    "        'max:', np.max(features), \n",
    "        'min:', np.min(features)\n",
    "    )\n",
    "    return features, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_model(X, y, \n",
    "                    folds, clf,\n",
    "                    vectorizer, ngram_range=(1, 1), \n",
    "                    max_df=.2, min_df=8, seed=2022):\n",
    "    scores = {}\n",
    "    roc_auc_scores = []\n",
    "    f1_scores = []\n",
    "    skf = StratifiedKFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "    for fold, (train_idxs, test_idxs) in enumerate(skf.split(X, y)):\n",
    "        \n",
    "        # train model\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_idxs], X.iloc[test_idxs]\n",
    "        y_train, y_test = y.iloc[train_idxs], y.iloc[test_idxs]\n",
    "        X_train, vectorizer = text_features(\n",
    "            X_train, \n",
    "            vectorizer=vectorizer\n",
    "        )\n",
    "        X_test = vectorizer.transform(X_test)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # saving models\n",
    "        # more about https://scikit-learn.org/stable/model_persistence.html\n",
    "        # NOTE - not only model, but vectorizer too!\n",
    "        \n",
    "        file_name = f'{MDLS_PATH}/model_fold_{fold}.joblib'\n",
    "        dump(clf, file_name)\n",
    "        print('saved to', file_name)\n",
    "        file_name = f'{MDLS_PATH}/vectorizer_fold_{fold}.joblib'\n",
    "        dump(vectorizer, file_name)\n",
    "        print('saved to', file_name)\n",
    "        \n",
    "        y_score = clf.predict_proba(X_test)\n",
    "        roc_auc_score_ = roc_auc_score(y_test, y_score[:, 1])\n",
    "        roc_auc_scores.append(roc_auc_score_)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        f1_score_ = f1_score(y_test, y_pred)\n",
    "\n",
    "        f1_scores.append(f1_score_)\n",
    "        msg = f'fold {fold} - val ROC-AUC score: {roc_auc_score_:.2f}, val f1-score: {f1_score_:.2f}'\n",
    "        print(msg)\n",
    "        \n",
    "        scores[f'fold {fold}'] = {\n",
    "            'roc_auc_scores': roc_auc_scores,\n",
    "            'f1_scores': f1_scores\n",
    "        }\n",
    "        \n",
    "        # WandB visualize all classifier plots\n",
    "        \n",
    "        run = wandb.init(\n",
    "            project='articles',\n",
    "            name=f'Articles classification {VER} fold {fold}',\n",
    "            settings=wandb.Settings(\n",
    "                start_method=\"thread\", \n",
    "                console=\"auto\"\n",
    "            )\n",
    "        )\n",
    "        wandb.config = CONFIG\n",
    "        wandb.sklearn.plot_classifier(\n",
    "            clf, \n",
    "            X_train, X_test, \n",
    "            y_train, y_test, \n",
    "            y_pred, y_score,\n",
    "            labels=['good', 'bad'],\n",
    "            is_binary=True,\n",
    "            model_name=f'{CONFIG[\"clf\"]} fold {fold}', \n",
    "            feature_names=vectorizer.get_feature_names_out()\n",
    "        )\n",
    "        \n",
    "        # metrics to WandB\n",
    "        wandb.summary['ROC AUC score'] = roc_auc_score_\n",
    "        wandb.summary['F1 score'] = f1_score_\n",
    "        \n",
    "        # WandB data versioning\n",
    "        data_path = f'./models_{CONFIG[\"version\"]}/data_fold_{fold}'\n",
    "        if not os.path.exists(data_path):\n",
    "            os.mkdir(data_path)\n",
    "        save_data_path = f'{data_path}/data_fold_{fold}.csv'\n",
    "        X.iloc[train_idxs].to_csv(save_data_path)\n",
    "        wandb_data = wandb.Artifact(f'dataset_{CONFIG[\"version\"]}_fold_{fold}', type='raw_data')\n",
    "        wandb_data.add_dir(data_path)\n",
    "        run.log_artifact(wandb_data)\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vectorizer with respect to\n",
    "# our configuration parameters\n",
    "vectorizer=TfidfVectorizer(\n",
    "    ngram_range=CONFIG['ngram_range'], \n",
    "    max_df=CONFIG['max_df'], \n",
    "    min_df=CONFIG['min_df']\n",
    ")\n",
    "\n",
    "# select the type of the model\n",
    "if CONFIG['clf'] == 'RandomForestClassifier':\n",
    "    clf = RandomForestClassifier(n_estimators=CONFIG['n_iters']) \n",
    "elif CONFIG['clf'] == 'GradientBoostingClassifier':\n",
    "    clf = GradientBoostingClassifier(n_estimators=CONFIG['n_iters']) \n",
    "else:\n",
    "    clf = LogisticRegression()\n",
    "    \n",
    "# start our training\n",
    "scores = cross_val_model(\n",
    "    X=df['proc'], \n",
    "    y=df['target'], \n",
    "    folds=CONFIG['folds'], \n",
    "    clf=clf,\n",
    "    vectorizer=vectorizer,\n",
    "    seed=CONFIG['seed']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-rings",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
